---
layout: distill
title: Reinforcement Learning-Based Model Matching in COBRA, a Slithering Snake Robot
description: byte-sized version of my master's thesis
giscus_comments: true
date: 2024-05-14
featured: true

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Abstract
  - name: Introduction
    subsections:
      - name: Simulation-to-Reality Gap
        # subsections:
        #   - name: Example Child Subsection 1
        #   - name: Example Child Subsection 2
      - name: Motivation
      - name: Simulators and Physics Engines
  - name: COBRA Platform
  - name: Central Pattern Generators
    subsections:
      - name: Modified Kuramoto Model
  - name: Model Matching Framework
    subsections:
      - name: Sidewinding Experiment
      - name: Methodology
  - name: Loco-manipulation Controller
    subsections:
      - name: Motivation
      - name: Methodology

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Abstract

This work employs a reinforcement learning-based model identification method aimed at
enhancing the accuracy of the dynamics for our snake robot, called COBRA. Leveraging gradient
information and iterative optimization, the proposed approach refines the parameters of COBRA’s
dynamical model such as coefficient of friction and actuator parameters using experimental and
simulated data. Experimental validation on the hardware platform demonstrates the efficacy of the
proposed approach, highlighting its potential to address sim-to-real gap in robot implementation.

---

## Introduction

## Simulation-to-Reality Gap

Developing effective locomotion controllers for mobile robots is challenging, and while Deep Reinforcement Learning (DRL) offers promising solutions, it requires impractical amounts of training data for high-risk tasks like locomotion and manipulation. Although computer simulations provide a safe and efficient training environment, the resulting policies often fail to transfer effectively to real hardware due to the "Simulation-to-Reality Gap" (sim2real gap), caused by discrepancies such as differences in friction, joint dynamics, and sensor noise between simulated and real environments. These discrepancies impede the transition of control policies from simulation to real-world applications, resulting in suboptimal performance and limited generalization of robotic systems.

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/sim2real_methods_new.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 1: Popular Solutions for Solving Sim2real Problem
</div>

To tackle the challenges of the Simulation-to-Reality Gap, recent research has focused on several advanced strategies. These include employing more sophisticated system identification procedures to improve model accuracy, training policies on real robots while simultaneously enhancing simulator accuracy through domain adaptation, and developing robust control policies that can generalize across varied simulated environments through domain randomization. Although these methods have shown good transferability of learned policies from simulation to real robots, they typically require at least 1000 hours of training in the simulator and/or hundreds of rollouts on the real robot. The figure below highlights several state-of-the-art solutions addressing the sim2real problem across various robots.

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/sota.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 2: Shows successful transfer of an RL-based policy on 1) ANYmal robot 2) & 6) Unitree A1 3) Hexapod 4) Cassie 5) Sirius 7) Mini Cheetah 8) 7-DoF Yumi Robot 9) & 13) ANYmal C 10) ANYmal B 11) Digit humanoid 12) Soft Snake Robot
</div>

---

## Motivation

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/research_gap.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 3: Highlights the shared aspects among the popular solutions to sim2real problem
</div>

Significant progress has been made in simulation-to-reality transfer for locomotion tasks, but several research gaps remain. There is a strong motivation to develop a framework which:

1. is data-efficient
2. is learning-based
3. is scalable
4. is modular
5. requires minimum rollouts on the real robot
6. has standardized evaluation metrics for sim2real transfer

---

## Simulators and Physics Engines

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/sim_table.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Table 1: Comparison of simulators
</div>

Table 1 compares pros and cons of most widely used physics simulators for robotics applications. After considering all the options, I chose the Webots simulator as the ideal platform for bridging the sim2real gap in training reinforcement learning (RL) agents for the COBRA platform due to its robust integration capabilities with RL training and extensive resources for developing locomotion, manipulation, and navigation controllers. Webots offers a user-friendly interface and supports various programming languages like Python and C++, simplifying the implementation and integration of RL algorithms. Its diverse library of robot models, realistic physics simulations, and advanced visualization tools create an optimal environment for training RL-based locomotion policies.

Additionally, Webots provides comprehensive tools and resources for developing controllers for various tasks essential to the COBRA platform’s functionality. Overall, Webots’ compatibility with RL training and its rich resources make it an excellent choice for addressing the sim2real gap in developing efficient controllers for the COBRA platform.

---

## COBRA Platform

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/fbd.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 4: Illustrates free body diagram of the COBRA platform
</div>

COBRA, short for Crater Observing Bio-inspired Rolling Articulator, is a snake robot inspired by the movement of serpentine creatures. Engineered to navigate rugged terrains like craters, where traditional robots may struggle, COBRA features 11 joints and 12 links, providing complex and versatile movements. Its 6 yawing and 5 pitching joints allow a wide range of orientations, making it suitable for diverse robotic applications. Figure 4 shows a free body diagram of COBRA and its ground friction coefficients. The ground contact model used in the simulator is given below:

$$
F_{GRF} = 
\begin{cases}
    0,                                & \text{if } p_{C,z} > 0\\
    [F_{GRF,x},F_{GRF,y},F_{GRF,z}]^T,& \text{otherwise}
\end{cases}
$$

$$
F_{GRF,i} = - s_{i} F_{GRF,z} \, \mathrm{sgn}(\dot p_{C,i}) - \mu_v \dot p_{C,i} ~~  \mbox{if} ~~i=x, y
$$

$$
F_{GRF,z} = -k_1 p_{C,z} - k_{2} \dot p_{C,z}
$$

$$
s_{i} = \Big(\mu_c - (\mu_c - \mu_s) \mathrm{exp} \left(-|\dot p_{C,i}|^2/v_s^2  \right) \Big)
$$

where $$ p_{C,i} $$, $$ i = x, y, z $$, are the $$ x - y - z $$ positions of the contact point; $$ F_{GRF,i} $$, $$ i = x, y, z $$, are the $$ x - y - z $$ components of the ground reaction force, assuming contact occurs between the robot and the ground substrate. The force terms are given by, $$ k_1 $$ and $$ k_2 $$ are the spring and damping coefficients of the compliant surface model. The term $$ s_i $$ is defined by $$ \mu_c $$, $$ \mu_s $$, and $$ \mu_v $$, which are the Coulomb, static, and viscous friction coefficients, with $$ v_s > 0 $$ being the Stribeck velocity. Specifically, the unknown parameters include actuator model parameters and Stribeck terms.

---

## Central Pattern Generators

Central Pattern Generators (CPGs) are neural networks in the spinal cord and brainstem of vertebrates, including humans, responsible for generating rhythmic neural activity that coordinates repetitive motor movements like walking, swimming, and breathing, without continuous input from higher brain centers. CPGs are crucial for locomotion and other rhythmic behaviors, producing rhythmic patterns even without sensory feedback or external stimuli. This autonomous rhythmicity allows for automatic and adaptive rhythmic movements without constant conscious control. 

In research and robotics, CPGs are studied for their potential in biomimetic control of locomotion and other behaviors in artificial systems. By modeling CPG principles in artificial neural networks, researchers can create control systems that mimic biological rhythmicity, integrating these artificial CPGs into robotic systems for autonomous and efficient control of locomotion, navigation, and other motor functions.

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/joint_traj_robot.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 5: Signals generated from a simple sine wave CPG
</div>

## Modified Kuramoto Model

The Kuramoto CPG model is a mathematical framework for describing rhythmic oscillatory behavior in biological systems, particularly for robotic locomotion control, by using coupled phase oscillators to generate synchronized and coordinated rhythmic patterns. I used a modified version of the original Kuramoto model, with the dynamics detailed below.

$$
\dot{\varphi} = \omega + A \cdot \varphi + B \cdot \theta
$$

$$
\ddot{r} = a \cdot [\frac{a}{4} \cdot (R - r) - \dot{r}]
$$

$$
x = r \cdot \sin(\varphi) + \delta
$$

$$
A =
\begin{bmatrix}
-\mu_1 & \mu_1 \\
\mu_2 & -2\mu_2 & \mu_2 \\
 &  & \ddots &  \\
 &  & \mu_{n-1} & -2\mu_{n-1} & \mu_{n-1} \\
&  & & \mu_n & -\mu_n \\
\end{bmatrix}
$$

$$
B =
\begin{bmatrix}
1 \\
-1 & 1 \\
& -1 & \ddots \\
& & \ddots & 1 \\
& & & -1 & 1 \\
& & & & -1 \\
\end{bmatrix}
$$

Where $$\varphi \in \mathbb{R}^n$$ and $$r \in \mathbb{R}^n$$ are the internal states of the CPG, $$n$$ is the number of output channels (usually equal to the number of robot joints), $$a$$ and $$\mu_i$$ are hyperparameters controlling the convergence rate, $$R \in \mathbb{R}^n$$, $$\omega \in \mathbb{R}^n$$, $$\theta \in \mathbb{R}^{n-1}$$, $$\delta \in \mathbb{R}^n$$ are inputs controlling the desired amplitude, frequency, phase shift, and offset, and $$x \in \mathbb{R}^n$$ is the output sinusoidal waves of $$n$$ channels.

---

## Model Matching Framework

## Sidewinding Experiment

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/real_untuned_text.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>
<div class="caption">
    Video 1: Highlights the sim2real gap oberserved between COBRA and Webots simulator. The robot performs sidewinding motion in webots with an untuned simulator model. The red ball represents the position attained by the actual robot when executing analogous joint trajectories
</div>

The right clip of Video 1 illustrates the COBRA platform sidewinding on flat ground. The sidewinding trajectory is generated by the sine equation:

$$
y(t) = A \sin(\omega t + \phi)
$$

where $$ y(t) $$ represents the lateral position of the COBRA platform at time $$ t, A $$ is the amplitude of the sidewinding motion, $$ \omega $$ is the angular frequency, and $$ \phi $$ is the phase angle. Figure 6 shows the data collected from the real robot.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/3d_traj_robot.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/joint_traj_robot.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 6: (Left) Illustrates positions of the head module, middle link and tail link are collected using OptiTrack system. (Right) Joint angle data (for all joints) is also collected while the robot is executing the sidewinding trajectory
</div>

The simulation-to-reality gap observed in the Webots simulator is highlighted in experiments conducted on the physical robot (Video 1). Motion capture technology recorded the trajectories of the head, tail, and middle links, along with joint trajectory data for all eleven joints. Odd-numbered joints executed pitching motions, while even-numbered joints executed yawing motions. A sidewinding gait was generated using a sinusoidal input signal to each joint, with adjustable parameters to modify the robot’s behavior. The input signal followed a sine-wave pattern with varying phase differences applied to each joint:

$$
A_{pitching} = 14^\circ, A_{yawing} = 60^\circ
$$

$$
\phi = \frac{\pi}{2} [0, 0, 1, 1, 2, 2, 3, 3, 0, 0, 1]
$$

The identical trajectory was also executed in the Webots simulator, allowing observation of the discrepancy between the simulator and the actual robot. This disparity is more evident in Figure 7, which compares the head trajectories over time for three different gait patterns in both the actual robot and the base simulator (untuned model). Gait 1 represents sidewinding at 0.35 Hz, Gait 2 at 0.5 Hz, and Gait 3 at 0.65 Hz. Additionally, Figure 8 highlights how the performance of the actuator model also contributes to the gap between simulation and reality. These disparities hinder the seamless transfer of control policies from simulation to the real robot, limiting the effectiveness of learned behaviors in practical applications. Addressing the sim-to-real issue is crucial for ensuring COBRA's reliability and robustness in diverse and dynamic environments.

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/sim2real_head_trajectories.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 7: Illustrates sim-to-real gap observed in COBRA’s head link trajectory while performing sidewinding gaits at frequencies of 0.35 Hz, 0.5 Hz, and 0.65 Hz respectively
</div>

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/sim2real_joint_angles.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 8: Illustrates sim-to-real gap observed in COBRA’s joint angles while performing sidewinding gaits at a frequencies of 0.5 Hz
</div>

---

## Methodology

The proposed model matching framework (Fig. 9) is designed to tune the dynamic model and identify accurate parameters that minimize the disparity between simulation and reality using a reinforcement learning-based method. Our methodology involved executing a predetermined sidewinding trajectory (operating at 0.5 Hz) on both the physical COBRA robot and the Webots simulator. A sidewinding trajectory command guided both systems through identical movements. Performance analysis was conducted in two domains: underactuated dynamics, which involved comparing the head's position and orientation to identify contact force discrepancies, and actuated dynamics, focusing on joint angle trajectories to evaluate movement precision. Positional data for the head, middle link, tail, and joints were collected during the sidewinding gait. Alignment between simulation and reality was quantified using a reward function, and a reinforcement learning agent adjusted model parameters to maximize this function, aiming to minimize the sim2real gap. Proximal Policy Optimization (PPO) algorithm is used to iteratively refine the simulation model.

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/model_matching.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 9: Model matching framework
</div>

The state $$ S_t $$ includes actuator parameters for internal tuning and Stribeck terms for external tuning. The actions $$ A_t $$ are adjustments to these simulation parameters.

$$
S = [h(\alpha), \bar{h}, q(\alpha), \bar{q} \mu, K_p, K_i, K_d]
$$

where $$ h(\alpha), \bar{h}, q(\alpha), \bar{q} $$ represent head trajectory from simulation, head trajectory from real robot, joint trajectories from simulation and joint trajectories from real robot. $$ K_p, K_i, K_d $$ represent PID gain values of the actuator model of the robot in simulator.

The reward function $$ R = R_{\text{int}} + R_{\text{ext}} $$ is defined as:

$$
R_{\text{external}} = - \sqrt{(x_{\text{des}} - x_{\text{actual}})^2 + (y_{\text{des}} - y_{\text{actual}})^2}
$$

where $$ x_{\text{des}} $$ and $$ y_{\text{des}} $$ are the desired x- and y-positions of the head module, and $$ x_{\text{actual}} $$ and $$ y_{\text{actual}} $$ are the actual positions from OptiTrack data. 

The internal reward $$ R_{\text{internal}} $$ is given by:

$$
R_{\text{internal}} = - (\phi_{\text{des}} - \phi_{\text{actual}})^2 - (\omega_{\text{des}} - \omega_{\text{actual}})^2 - (A_{\text{des}} - A_{\text{actual}})^2
$$

where $$ \phi $$, $$ \omega $$, and $$ A $$ are CPG variables. These reward functions are designed to penalize deviations in both passive and actuated dynamics, using the L$^2$-norm of the differences in joint angle trajectories and the final head position.

During the training phase, the policy network is updated with new data, with PPO adjusting the policy to maximize cumulative rewards while preserving similarity to the previous policy. This is achieved through a mechanism known as clipping, which prevents drastic policy changes. The results of the tuning procedure are presented in the results section.

---

## Loco-manipulation Controller

## Motivation

The tuned model derived from the Model Matching framework provides valuable capabilities for developing advanced locomotion strategies that can be transferred to the real robot. One such strategy is Loco-manipulation, which involves manipulating objects in the robot's vicinity during locomotion tasks. This technique is especially significant for the COBRA platform, leveraging its unique design to enhance dexterity and precision in object manipulation. By integrating locomotion and manipulation seamlessly, COBRA can increase its functionality and adaptability across various tasks, boosting overall efficiency and versatility. In the following sections, I introduce the Loco-manipulation controller, detailing its design and functionality. Although this controller addresses the specified problem effectively, it is important to note that the current version cannot be directly transferred to the real robot because the locomotion policy was trained using an untuned model. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/locoman.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>
<div class="caption">
    Video 2: Shows COBRA performing loco-manipulation
</div>

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/locoman_sim.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 9: Loco-manipulation problem setup in Webots simulator
</div>

---

## Methodology

The hierarchical framework for developing the loco-manipulation controller is illustrated in Fig. 10. This structure simplifies the problem by dividing it into two parts, detailed step-by-step in Algorithm 1. The high-level manipulation path planner (steps 1 and 2 in the algorithm) plans the trajectory from the robot's head position to designated goal positions, ensuring the robot can effectively manipulate an object from its initial location to the target. This planner creates waypoints along the path and sends them to the low-level locomotion controller (steps 3 and 4 in algorithm). The low-level locomotion controller receives these waypoints and executes them using a locomotion policy trained with reinforcement learning. This policy generates parameters for the modified kuramoto CPG model, which determines the joint positions necessary for the robot's movements. Results and analysis of the loco-manipulation controller are presented in results section.

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/locomancon.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Figure 9: Hierarchical structure of loco-manipulation controller
</div>

<div class="row mt-3">
    {% include figure.liquid loading="eager" path="assets/img/locoman_algo.png" class="img-fluid rounded z-depth-1" %}
</div>
<div class="caption">
    Algorithm 1: Modified pure pursuit algorithm for loco-manipulation
</div>

---
