<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning-Based Model Matching in COBRA, a Slithering Snake Robot | Harin Kumar Nallaguntla </title> <meta name="author" content="Harin Kumar Nallaguntla"> <meta name="description" content="byte-sized version of my master's thesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zer0sh0t.github.io/blog/2024/my-thesis/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Reinforcement Learning-Based Model Matching in COBRA, a Slithering Snake Robot",
            "description": "byte-sized version of my master's thesis",
            "published": "May 14, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Harin Kumar</span> Nallaguntla </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Reinforcement Learning-Based Model Matching in COBRA, a Slithering Snake Robot</h1> <p>byte-sized version of my master's thesis</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#simulation-to-reality-gap">Simulation-to-Reality Gap</a> </li> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#simulators-and-physics-engines">Simulators and Physics Engines</a> </li> </ul> <div> <a href="#cobra-platform">COBRA Platform</a> </div> <div> <a href="#central-pattern-generators">Central Pattern Generators</a> </div> <ul> <li> <a href="#modified-kuramoto-model">Modified Kuramoto Model</a> </li> </ul> <div> <a href="#model-matching-framework">Model Matching Framework</a> </div> <ul> <li> <a href="#sidewinding-experiment">Sidewinding Experiment</a> </li> <li> <a href="#methodology">Methodology</a> </li> </ul> <div> <a href="#loco-manipulation-controller">Loco-manipulation Controller</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> <li> <a href="#methodology">Methodology</a> </li> </ul> </nav> </d-contents> <h2 id="abstract">Abstract</h2> <p>This work employs a reinforcement learning-based model identification method aimed at enhancing the accuracy of the dynamics for our snake robot, called COBRA. Leveraging gradient information and iterative optimization, the proposed approach refines the parameters of COBRA’s dynamical model such as coefficient of friction and actuator parameters using experimental and simulated data. Experimental validation on the hardware platform demonstrates the efficacy of the proposed approach, highlighting its potential to address sim-to-real gap in robot implementation.</p> <hr> <h2 id="introduction">Introduction</h2> <h2 id="simulation-to-reality-gap">Simulation-to-Reality Gap</h2> <p>Developing effective locomotion controllers for mobile robots is challenging, and while Deep Reinforcement Learning (DRL) offers promising solutions, it requires impractical amounts of training data for high-risk tasks like locomotion and manipulation. Although computer simulations provide a safe and efficient training environment, the resulting policies often fail to transfer effectively to real hardware due to the “Simulation-to-Reality Gap” (sim2real gap), caused by discrepancies such as differences in friction, joint dynamics, and sensor noise between simulated and real environments. These discrepancies impede the transition of control policies from simulation to real-world applications, resulting in suboptimal performance and limited generalization of robotic systems.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sim2real_methods_new-480.webp 480w,/assets/img/sim2real_methods_new-800.webp 800w,/assets/img/sim2real_methods_new-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sim2real_methods_new.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 1: Popular Solutions for Solving Sim2real Problem </div> <p>To tackle the challenges of the Simulation-to-Reality Gap, recent research has focused on several advanced strategies. These include employing more sophisticated system identification procedures to improve model accuracy, training policies on real robots while simultaneously enhancing simulator accuracy through domain adaptation, and developing robust control policies that can generalize across varied simulated environments through domain randomization. Although these methods have shown good transferability of learned policies from simulation to real robots, they typically require at least 1000 hours of training in the simulator and/or hundreds of rollouts on the real robot. The figure below highlights several state-of-the-art solutions addressing the sim2real problem across various robots.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sota-480.webp 480w,/assets/img/sota-800.webp 800w,/assets/img/sota-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sota.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 2: Shows successful transfer of an RL-based policy on 1) ANYmal robot 2) &amp; 6) Unitree A1 3) Hexapod 4) Cassie 5) Sirius 7) Mini Cheetah 8) 7-DoF Yumi Robot 9) &amp; 13) ANYmal C 10) ANYmal B 11) Digit humanoid 12) Soft Snake Robot </div> <hr> <h2 id="motivation">Motivation</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_gap-480.webp 480w,/assets/img/research_gap-800.webp 800w,/assets/img/research_gap-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research_gap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 3: Highlights the shared aspects among the popular solutions to sim2real problem </div> <p>Significant progress has been made in simulation-to-reality transfer for locomotion tasks, but several research gaps remain. There is a strong motivation to develop a framework which:</p> <ol> <li>is data-efficient</li> <li>is learning-based</li> <li>is scalable</li> <li>is modular</li> <li>requires minimum rollouts on the real robot</li> <li>has standardized evaluation metrics for sim2real transfer</li> </ol> <hr> <h2 id="simulators-and-physics-engines">Simulators and Physics Engines</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sim_table-480.webp 480w,/assets/img/sim_table-800.webp 800w,/assets/img/sim_table-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sim_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 1: Comparison of simulators </div> <p>Table 1 compares pros and cons of most widely used physics simulators for robotics applications. After considering all the options, I chose the Webots simulator as the ideal platform for bridging the sim2real gap in training reinforcement learning (RL) agents for the COBRA platform due to its robust integration capabilities with RL training and extensive resources for developing locomotion, manipulation, and navigation controllers. Webots offers a user-friendly interface and supports various programming languages like Python and C++, simplifying the implementation and integration of RL algorithms. Its diverse library of robot models, realistic physics simulations, and advanced visualization tools create an optimal environment for training RL-based locomotion policies.</p> <p>Additionally, Webots provides comprehensive tools and resources for developing controllers for various tasks essential to the COBRA platform’s functionality. Overall, Webots’ compatibility with RL training and its rich resources make it an excellent choice for addressing the sim2real gap in developing efficient controllers for the COBRA platform.</p> <hr> <h2 id="cobra-platform">COBRA Platform</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fbd-480.webp 480w,/assets/img/fbd-800.webp 800w,/assets/img/fbd-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/fbd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 4: Illustrates free body diagram of the COBRA platform </div> <p>COBRA, short for Crater Observing Bio-inspired Rolling Articulator, is a snake robot inspired by the movement of serpentine creatures. Engineered to navigate rugged terrains like craters, where traditional robots may struggle, COBRA features 11 joints and 12 links, providing complex and versatile movements. Its 6 yawing and 5 pitching joints allow a wide range of orientations, making it suitable for diverse robotic applications. Figure 4 shows a free body diagram of COBRA and its ground friction coefficients. The ground contact model used in the simulator is given below:</p> \[F_{GRF} = \begin{cases} 0, &amp; \text{if } p_{C,z} &gt; 0\\ [F_{GRF,x},F_{GRF,y},F_{GRF,z}]^T,&amp; \text{otherwise} \end{cases}\] \[F_{GRF,i} = - s_{i} F_{GRF,z} \, \mathrm{sgn}(\dot p_{C,i}) - \mu_v \dot p_{C,i} ~~ \mbox{if} ~~i=x, y\] \[F_{GRF,z} = -k_1 p_{C,z} - k_{2} \dot p_{C,z}\] \[s_{i} = \Big(\mu_c - (\mu_c - \mu_s) \mathrm{exp} \left(-|\dot p_{C,i}|^2/v_s^2 \right) \Big)\] <p>where \(p_{C,i}\), \(i = x, y, z\), are the \(x - y - z\) positions of the contact point; \(F_{GRF,i}\), \(i = x, y, z\), are the \(x - y - z\) components of the ground reaction force, assuming contact occurs between the robot and the ground substrate. The force terms are given by, \(k_1\) and \(k_2\) are the spring and damping coefficients of the compliant surface model. The term \(s_i\) is defined by \(\mu_c\), \(\mu_s\), and \(\mu_v\), which are the Coulomb, static, and viscous friction coefficients, with \(v_s &gt; 0\) being the Stribeck velocity. Specifically, the unknown parameters include actuator model parameters and Stribeck terms.</p> <hr> <h2 id="central-pattern-generators">Central Pattern Generators</h2> <p>Central Pattern Generators (CPGs) are neural networks in the spinal cord and brainstem of vertebrates, including humans, responsible for generating rhythmic neural activity that coordinates repetitive motor movements like walking, swimming, and breathing, without continuous input from higher brain centers. CPGs are crucial for locomotion and other rhythmic behaviors, producing rhythmic patterns even without sensory feedback or external stimuli. This autonomous rhythmicity allows for automatic and adaptive rhythmic movements without constant conscious control.</p> <p>In research and robotics, CPGs are studied for their potential in biomimetic control of locomotion and other behaviors in artificial systems. By modeling CPG principles in artificial neural networks, researchers can create control systems that mimic biological rhythmicity, integrating these artificial CPGs into robotic systems for autonomous and efficient control of locomotion, navigation, and other motor functions.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/joint_traj_robot-480.webp 480w,/assets/img/joint_traj_robot-800.webp 800w,/assets/img/joint_traj_robot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/joint_traj_robot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 5: Signals generated from a simple sine wave CPG </div> <h2 id="modified-kuramoto-model">Modified Kuramoto Model</h2> <p>The Kuramoto CPG model is a mathematical framework for describing rhythmic oscillatory behavior in biological systems, particularly for robotic locomotion control, by using coupled phase oscillators to generate synchronized and coordinated rhythmic patterns. I used a modified version of the original Kuramoto model, with the dynamics detailed below.</p> \[\dot{\varphi} = \omega + A \cdot \varphi + B \cdot \theta\] \[\ddot{r} = a \cdot [\frac{a}{4} \cdot (R - r) - \dot{r}]\] \[x = r \cdot \sin(\varphi) + \delta\] \[A = \begin{bmatrix} -\mu_1 &amp; \mu_1 \\ \mu_2 &amp; -2\mu_2 &amp; \mu_2 \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; \mu_{n-1} &amp; -2\mu_{n-1} &amp; \mu_{n-1} \\ &amp; &amp; &amp; \mu_n &amp; -\mu_n \\ \end{bmatrix}\] \[B = \begin{bmatrix} 1 \\ -1 &amp; 1 \\ &amp; -1 &amp; \ddots \\ &amp; &amp; \ddots &amp; 1 \\ &amp; &amp; &amp; -1 &amp; 1 \\ &amp; &amp; &amp; &amp; -1 \\ \end{bmatrix}\] <p>Where \(\varphi \in \mathbb{R}^n\) and \(r \in \mathbb{R}^n\) are the internal states of the CPG, \(n\) is the number of output channels (usually equal to the number of robot joints), \(a\) and \(\mu_i\) are hyperparameters controlling the convergence rate, \(R \in \mathbb{R}^n\), \(\omega \in \mathbb{R}^n\), \(\theta \in \mathbb{R}^{n-1}\), \(\delta \in \mathbb{R}^n\) are inputs controlling the desired amplitude, frequency, phase shift, and offset, and \(x \in \mathbb{R}^n\) is the output sinusoidal waves of \(n\) channels.</p> <hr> <h2 id="model-matching-framework">Model Matching Framework</h2> <h2 id="sidewinding-experiment">Sidewinding Experiment</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/real_untuned_text.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Video 1: Highlights the sim2real gap oberserved between COBRA and Webots simulator. The robot performs sidewinding motion in webots with an untuned simulator model. The red ball represents the position attained by the actual robot when executing analogous joint trajectories </div> <p>The right clip of Video 1 illustrates the COBRA platform sidewinding on flat ground. The sidewinding trajectory is generated by the sine equation:</p> \[y(t) = A \sin(\omega t + \phi)\] <p>where \(y(t)\) represents the lateral position of the COBRA platform at time \(t, A\) is the amplitude of the sidewinding motion, \(\omega\) is the angular frequency, and \(\phi\) is the phase angle. Figure 6 shows the data collected from the real robot.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3d_traj_robot-480.webp 480w,/assets/img/3d_traj_robot-800.webp 800w,/assets/img/3d_traj_robot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3d_traj_robot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/joint_traj_robot-480.webp 480w,/assets/img/joint_traj_robot-800.webp 800w,/assets/img/joint_traj_robot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/joint_traj_robot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: (Left) Illustrates positions of the head module, middle link and tail link are collected using OptiTrack system. (Right) Joint angle data (for all joints) is also collected while the robot is executing the sidewinding trajectory </div> <p>The simulation-to-reality gap observed in the Webots simulator is highlighted in experiments conducted on the physical robot (Video 1). Motion capture technology recorded the trajectories of the head, tail, and middle links, along with joint trajectory data for all eleven joints. Odd-numbered joints executed pitching motions, while even-numbered joints executed yawing motions. A sidewinding gait was generated using a sinusoidal input signal to each joint, with adjustable parameters to modify the robot’s behavior. The input signal followed a sine-wave pattern with varying phase differences applied to each joint:</p> \[A_{pitching} = 14^\circ, A_{yawing} = 60^\circ\] \[\phi = \frac{\pi}{2} [0, 0, 1, 1, 2, 2, 3, 3, 0, 0, 1]\] <p>The identical trajectory was also executed in the Webots simulator, allowing observation of the discrepancy between the simulator and the actual robot. This disparity is more evident in Figure 7, which compares the head trajectories over time for three different gait patterns in both the actual robot and the base simulator (untuned model). Gait 1 represents sidewinding at 0.35 Hz, Gait 2 at 0.5 Hz, and Gait 3 at 0.65 Hz. Additionally, Figure 8 highlights how the performance of the actuator model also contributes to the gap between simulation and reality. These disparities hinder the seamless transfer of control policies from simulation to the real robot, limiting the effectiveness of learned behaviors in practical applications. Addressing the sim-to-real issue is crucial for ensuring COBRA’s reliability and robustness in diverse and dynamic environments.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sim2real_head_trajectories-480.webp 480w,/assets/img/sim2real_head_trajectories-800.webp 800w,/assets/img/sim2real_head_trajectories-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sim2real_head_trajectories.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 7: Illustrates sim-to-real gap observed in COBRA’s head link trajectory while performing sidewinding gaits at frequencies of 0.35 Hz, 0.5 Hz, and 0.65 Hz respectively </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sim2real_joint_angles-480.webp 480w,/assets/img/sim2real_joint_angles-800.webp 800w,/assets/img/sim2real_joint_angles-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sim2real_joint_angles.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 8: Illustrates sim-to-real gap observed in COBRA’s joint angles while performing sidewinding gaits at a frequencies of 0.5 Hz </div> <hr> <h2 id="methodology">Methodology</h2> <p>The proposed model matching framework (Fig. 9) is designed to tune the dynamic model and identify accurate parameters that minimize the disparity between simulation and reality using a reinforcement learning-based method. Our methodology involved executing a predetermined sidewinding trajectory (operating at 0.5 Hz) on both the physical COBRA robot and the Webots simulator. A sidewinding trajectory command guided both systems through identical movements. Performance analysis was conducted in two domains: underactuated dynamics, which involved comparing the head’s position and orientation to identify contact force discrepancies, and actuated dynamics, focusing on joint angle trajectories to evaluate movement precision. Positional data for the head, middle link, tail, and joints were collected during the sidewinding gait. Alignment between simulation and reality was quantified using a reward function, and a reinforcement learning agent adjusted model parameters to maximize this function, aiming to minimize the sim2real gap. Proximal Policy Optimization (PPO) algorithm is used to iteratively refine the simulation model.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_matching-480.webp 480w,/assets/img/model_matching-800.webp 800w,/assets/img/model_matching-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/model_matching.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 9: Model matching framework </div> <p>The state \(S_t\) includes actuator parameters for internal tuning and Stribeck terms for external tuning. The actions \(A_t\) are adjustments to these simulation parameters.</p> \[S = [h(\alpha), \bar{h}, q(\alpha), \bar{q} \mu, K_p, K_i, K_d]\] <p>where \(h(\alpha), \bar{h}, q(\alpha), \bar{q}\) represent head trajectory from simulation, head trajectory from real robot, joint trajectories from simulation and joint trajectories from real robot. \(K_p, K_i, K_d\) represent PID gain values of the actuator model of the robot in simulator.</p> <p>The reward function \(R = R_{\text{int}} + R_{\text{ext}}\) is defined as:</p> \[R_{\text{external}} = - \sqrt{(x_{\text{des}} - x_{\text{actual}})^2 + (y_{\text{des}} - y_{\text{actual}})^2}\] <p>where \(x_{\text{des}}\) and \(y_{\text{des}}\) are the desired x- and y-positions of the head module, and \(x_{\text{actual}}\) and \(y_{\text{actual}}\) are the actual positions from OptiTrack data.</p> <p>The internal reward \(R_{\text{internal}}\) is given by:</p> \[R_{\text{internal}} = - (\phi_{\text{des}} - \phi_{\text{actual}})^2 - (\omega_{\text{des}} - \omega_{\text{actual}})^2 - (A_{\text{des}} - A_{\text{actual}})^2\] <p>where \(\phi\), \(\omega\), and \(A\) are CPG variables. These reward functions are designed to penalize deviations in both passive and actuated dynamics, using the L$^2$-norm of the differences in joint angle trajectories and the final head position.</p> <p>During the training phase, the policy network is updated with new data, with PPO adjusting the policy to maximize cumulative rewards while preserving similarity to the previous policy. This is achieved through a mechanism known as clipping, which prevents drastic policy changes. The results of the tuning procedure are presented in the results section.</p> <hr> <h2 id="loco-manipulation-controller">Loco-manipulation Controller</h2> <h2 id="motivation-1">Motivation</h2> <p>The tuned model derived from the Model Matching framework provides valuable capabilities for developing advanced locomotion strategies that can be transferred to the real robot. One such strategy is Loco-manipulation, which involves manipulating objects in the robot’s vicinity during locomotion tasks. This technique is especially significant for the COBRA platform, leveraging its unique design to enhance dexterity and precision in object manipulation. By integrating locomotion and manipulation seamlessly, COBRA can increase its functionality and adaptability across various tasks, boosting overall efficiency and versatility. In the following sections, I introduce the Loco-manipulation controller, detailing its design and functionality. Although this controller addresses the specified problem effectively, it is important to note that the current version cannot be directly transferred to the real robot because the locomotion policy was trained using an untuned model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/locoman.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Video 2: Shows COBRA performing loco-manipulation </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/locoman_sim-480.webp 480w,/assets/img/locoman_sim-800.webp 800w,/assets/img/locoman_sim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/locoman_sim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 9: Loco-manipulation problem setup in Webots simulator </div> <hr> <h2 id="methodology-1">Methodology</h2> <p>The hierarchical framework for developing the loco-manipulation controller is illustrated in Fig. 10. This structure simplifies the problem by dividing it into two parts, detailed step-by-step in Algorithm 1. The high-level manipulation path planner (steps 1 and 2 in the algorithm) plans the trajectory from the robot’s head position to designated goal positions, ensuring the robot can effectively manipulate an object from its initial location to the target. This planner creates waypoints along the path and sends them to the low-level locomotion controller (steps 3 and 4 in algorithm). The low-level locomotion controller receives these waypoints and executes them using a locomotion policy trained with reinforcement learning. This policy generates parameters for the modified kuramoto CPG model, which determines the joint positions necessary for the robot’s movements. Results and analysis of the loco-manipulation controller are presented in results section.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/locomancon-480.webp 480w,/assets/img/locomancon-800.webp 800w,/assets/img/locomancon-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/locomancon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Figure 9: Loco-manipulation controller </div> <pre><code class="language-psuedocode">\begin{algorithm}
\caption{Modified Pure Pursuit Algorithm for Loco-manipulation}
\label{algo:locomanipulation}
\begin{algorithmic}[1]
\STATE \textbf{Given:} head coordinates $(head_x, head_y)$, box coordinates $(box_x, box_y)$, goal coordinates $(goal_x, goal_y)$
\STATE \textbf{Initialize:} Thresholds $\delta_{hb}$ and $\delta_{bg}$, step length $L$, waypoints count $n$, width $W$

\PROCEDURE{NavigateToTarget}{}
    \STATE \textbf{Step 1:} Select the target based on distance
    \IF {$L2(\text{head}, \text{box}) &lt; \delta_{hb}$}
        \STATE $target \gets box$
    \ELSE
        \STATE $target \gets goal$
    \ENDIF

    \STATE \textbf{Step 2:} Generate $n$ waypoints between head and target
    \FOR {$i \gets 1$ to $n$}
        \STATE $x_i \gets head_x + \frac{i}{n+1}(target_x - head_x)$
        \STATE $y_i \gets head_y + \frac{i}{n+1}(target_y - head_y)$
    \ENDFOR

    \STATE \textbf{Step 3:} Take one step in the direction of the look ahead vector using the RL-based locomotion policy
    \STATE $\vec{P}_{lookahead} \gets \vec{P}_{current} + \frac{L \cdot \vec{H}}{\|\vec{H}\|}$
    \STATE $\alpha \gets \arctan{\left(\frac{2LW}{L2(\text{head}, \text{target})}\right)}$

    \STATE \textbf{Step 4:} Repeat until the box is close to the goal
    \WHILE {$L2(\text{box}, \text{goal}) \geq \delta_{bg}$}
        \STATE Go back to \textbf{Step 1}
    \ENDWHILE
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Harin Kumar Nallaguntla. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>